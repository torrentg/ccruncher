
%***************************************************************************
%
% CreditCruncher - A portfolio credit risk valorator
% Copyright (C) 2004 Gerard Torrent
%
% This program is free software; you can redistribute it and/or
% modify it under the terms of the GNU General Public License
% as published by the Free Software Foundation; either version 2
% of the License.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.
%
% You should have received a copy of the GNU General Public License
% along with this program; if not, write to the Free Software
% Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
%
%
% appendices.tex - TeX documentation file
% --------------------------------------------------------------------------
%
% 2005/01/22 - Gerard Torrent [gerard@fobos.generacio.com]
%   . initial release
%
%***************************************************************************

\chapter{Ap\'endices}
\label{sec:apendixes}

%---------------------------------------------------------------------------

\section{Conceptos b\'asicos de estad\'istica}

\paragraph{Esperanza.} Definimos la esperanza de una variable aleatoria 
discreta de la forma siguiente:
\begin{displaymath}
E(X) = \sum_{i} i \cdot P(X=i)
\end{displaymath}
En el caso de una variable aleatoria continua con funci\'on de distribuci\'on 
$f(x)$ la esperanza se expresa como:
\begin{displaymath}
E(X) = \int_{-\infty}^{\infty} x \cdot f(x) dx
\end{displaymath}

\paragraph{Varianza.} Definimos la varianza de una variable aleatoria discreta 
de la forma siguiente:
\begin{displaymath}
Var(X) = \sum_{i} (i-E(X))^2 \cdot P(X=i)
\end{displaymath}
En el caso de una variable aleatoria continua con funci\'on de distribuci\'on 
$f(x)$ la varianza se expresa como:
\begin{displaymath}
Var(X) = \int_{-\infty}^{\infty} (x-E(X))^2 \cdot f(x) dx
\end{displaymath}

\paragraph{Covarianza.} Definimos la covarianza entre dos variables 
aleatorias $X$ e $Y$ de la forma siguiente:
\begin{displaymath}
Cov(X,Y) = E(X \cdot Y) - E(X) \cdot E(Y)
\end{displaymath}

\paragraph{Correlaci\'on.} Definimos la correlaci\'on, $\rho$, entre dos 
variables aleatorias $X$ e $Y$ de la forma siguiente:
\begin{displaymath}
\rho_{X,Y} = \frac{Cov(X,Y)}{\sqrt{Var(x)+Var(Y)}}
\end{displaymath}


%---------------------------------------------------------------------------

\section{La variable aleatoria de Bernoulli}

\paragraph{Definici\'on.} La variable aleatoria discreta Bernouilli, $X$, se 
utiliza para modelar fen\'omenos que solamente pueden tomar dos estados, 
$0$ y $1$, con probabilidades $p$ y $(1-p)$ respectivamente. La notaremos 
como $X \sim Ber(p)$:
\begin{displaymath}
P(X=0) = (1 - p) \qquad   P(X=1) = p \qquad p \in [0,1]
\end{displaymath}
 
\paragraph{Esperanza.} La esperanza de una variable aleatoria Bernouilli $X \sim Ber(p)$ 
es $p$. Este resultado es la aplicaci\'on directa de la definici\'on de esperanza 
para una variable aleatoria discreta:
\begin{displaymath}
E(X) = \sum_{i} i \cdot P(X=i) = 1 \cdot p + 0 \cdot (1-p) = p
\end{displaymath}

\paragraph{Varianza.} La varianza de una variable aleatoria Bernouilli $X \sim Ber(p)$ 
es $p \cdot (1-p)$. Este resultado es la aplicaci\'on directa de la definici\'on 
de varianza para una variable aleatoria discreta:
\begin{displaymath}
Var(X)= \sum_{i} (i-E(X))^2 \cdot P(X=i) = (1-p)^2 \cdot p + (-p)^2 \cdot (1-p) = p \cdot (1-p)
\end{displaymath}
 
\paragraph{Simulaci\'on.} La simulaci\'on de una variable Bernouilli 
$X \sim Ber(p)$ la realizamos de la siguiente forma:
\begin{displaymath}
x= \left\{
\begin{array}{cc}
0 & u \in [0,1-p) \cr
1 & u \in [1-p,1]
\end{array}
\right.
\qquad u \sim U[0,1]
\end{displaymath}

%---------------------------------------------------------------------------

\section{La variable aleatoria Binomial}

\paragraph{Definici\'on.} La suma de $n$ variables aleatorias Bernoulli 
independientes e id\'enticamente distribuidas es una variable aleatoria discreta, 
$X$ que llamamos Binomial, $X \sim B(n,p)$.
\begin{displaymath}
P(X=k) = {n \choose k} \cdot p^k \cdot (1-p)^{n-k} \qquad {n \choose k} = \frac{n!}{k! \cdot (n-k)!}
\qquad k \in \{0, \cdots, n\}
\end{displaymath}

\paragraph{Esperanza.} La esperanza de una variable aleatoria Binomial 
$X \sim B(n,p)$ es:
\begin{displaymath}
E(X) = n \cdot p
\end{displaymath}

\paragraph{Varianza.} La varianza de una variable aleatoria Binomial 
$X \sim B(n,p)$ es:
\begin{displaymath}
Var(X)= n \cdot p \cdot (1-p)
\end{displaymath}

\paragraph{Proposici\'on.} El Teorema Central del L\'imite nos permite, en el
caso de $n$ grandes, aproximar la distribuci\'on discreta Binomial por una 
distribuci\'on continua Normal:
\begin{displaymath}
B(n,p) \approx N(n \cdot p, \sqrt{n \cdot p \cdot (1-p)})
\end{displaymath}


%---------------------------------------------------------------------------

\section{La variable aleatoria Normal}

\subsection{Definici\'on y propiedades}

\begin{displaymath}
P(X \leq x) = \Phi(x) = \int_{-\infty}^{x} \frac{e^{-t^2}}{\sqrt{2 \pi}} dt
\end{displaymath}

\subsection{Simulaci\'on}

Para la generaci\'on de una realizaci\'on, $z$, de una variable aleatoria normal  
$Z \sim N(\mu, \sigma)$ utilizamos el siguiente algoritmo:

\begin{displaymath}
z = \mu + \sigma\cdot \sqrt{-2 \cdot ln(u_1[0,1])} \cdot cos(2 \cdot \pi \cdot u_2[0,1])
\end{displaymath}

\noindent donde $u_1[0,1]$ y $u_2[0,1]$ son realizaciones de una variable aleatoria uniforme 
en el intervalo $[0,1]$.

%---------------------------------------------------------------------------

\section{C\'alculo de la raiz de una matriz}

\paragraph{Definici\'on.}
Diremos que 2 matrices $A$ y $B$ de orden $n$ son semejantes si existe una 
matriz, $P$, de orden $n$ con $det(P) \neq 0$ tal que 
$B = P^{-1} \cdot A \cdot P$.


\paragraph{Proposici\'on.} Si dos matrices $A$ y $B$ son semejantes 
($B = P^{-1} \cdot A \cdot P$) entonces:
\begin{displaymath}
det(A) = det(B)
\end{displaymath}
\begin{displaymath}
B^n = P^{-1} \cdot A^{n} \cdot P
\end{displaymath}

\paragraph{Definici\'on.} 
Diremos que una matriz $A$ de orden $n$ es diagonalizable si es semejante a una 
matriz diagonal $D$, o sea, $A = P^{-1} \cdot D \cdot P$ siendo $det(D) \neq 0$.

\paragraph{Proposici\'on.} 
Para que una matriz $A$ sea diagonalizable es necesario y suficiente que:
\begin{itemize}
\item Los valores propios de $A$ sean todos reales
\item Los $n$ vectores propios de $A$ sean independientes
\end{itemize}

\paragraph{Proposici\'on.}
Si una matriz $A$ es diagonalizable ($A = P^{-1} \cdot D \cdot P$) entonces: 
\begin{itemize}
\item $D$ es una matriz diagonal compuesta por los valores propios de la matriz $A$
\item $P$ es la matriz formada por los vectores propios de la matriz $A$
\end{itemize}

\paragraph{Resultado.}
Sea $A$ la ra\'iz $n$-esima de una matriz diagonalizable $B$. Entonces:
\begin{displaymath}
A^n = B = P^{-1} \cdot D \cdot P 
\Longrightarrow  
A = \sqrt[n]{B} = P^{-1} \cdot \sqrt[n]{D} \cdot P
\end{displaymath} 

