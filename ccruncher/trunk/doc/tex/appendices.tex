
%***************************************************************************
%
% CreditCruncher - A portfolio credit risk valorator
% Copyright (C) 2004 Gerard Torrent
%
% This program is free software; you can redistribute it and/or
% modify it under the terms of the GNU General Public License
% as published by the Free Software Foundation; either version 2
% of the License.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.
%
% You should have received a copy of the GNU General Public License
% along with this program; if not, write to the Free Software
% Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
%
%
% appendices.tex - TeX documentation file
% --------------------------------------------------------------------------
%
% 2005/01/22 - Gerard Torrent [gerard@fobos.generacio.com]
%   . initial release
%
%***************************************************************************

\chapter{Ap\'endices}
\label{sec:apendixes}

%---------------------------------------------------------------------------
\section{Conceptos b\'asicos de estad\'istica}
\label{apendix:stats}

\paragraph{Definici\'on.} Llamamos funci\'on de distribuci\'on o cdf de la
variable aleatoria $X$ a la funci\'on $F$ que cumple:
\begin{displaymath}
F(x) = P(X \leq x)
\end{displaymath}

\paragraph{Definici\'on.} Llamamos funci\'on de probabilidad o densidad o pdf
 de la variable aleatoria $X$ a la funci\'on $f$ que cumple:
\begin{displaymath}
f(x) = \int_{-\infty}^{x} f(t) dt
\end{displaymath}

\paragraph{Proposici\'on.} Sea $X$ una variable aleatoria continua con
funci\'on de densidad $f_X(x)$. La funci\'on de densidad de $Y=H(X)$ siendo 
$H(.)$ mon\'otona (estrictamente creciente o estrictamente decreciente) es:
\begin{displaymath}
f_Y(y) = f_X(H^{-1}(y))\cdot \left| \frac{d(H^{-1}(y))}{dy} \right|
\end{displaymath}

\paragraph{Esperanza.} Definimos la esperanza de una variable aleatoria 
discreta de la forma siguiente:
\begin{displaymath}
E(X) = \sum_{i} i \cdot P(X=i)
\end{displaymath}
En el caso de una variable aleatoria continua con funci\'on de distribuci\'on 
$f(x)$ la esperanza se expresa como:
\begin{displaymath}
E(X) = \int_{-\infty}^{\infty} x \cdot f(x) dx
\end{displaymath}

\paragraph{Varianza.} Definimos la varianza de una variable aleatoria discreta 
de la forma siguiente:
\begin{displaymath}
Var(X) = \sum_{i} (i-E(X))^2 \cdot P(X=i)
\end{displaymath}
En el caso de una variable aleatoria continua con funci\'on de distribuci\'on 
$f(x)$ la varianza se expresa como:
\begin{displaymath}
Var(X) = \int_{-\infty}^{\infty} (x-E(X))^2 \cdot f(x) dx
\end{displaymath}

\paragraph{Covarianza.} Definimos la covarianza entre dos variables 
aleatorias $X$ e $Y$ de la forma siguiente:
\begin{displaymath}
Cov(X,Y) = E(X \cdot Y) - E(X) \cdot E(Y)
\end{displaymath}

\paragraph{Correlaci\'on.} Definimos la correlaci\'on, $\rho$, entre dos 
variables aleatorias $X$ e $Y$ de la forma siguiente:
\begin{displaymath}
\rho_{X,Y} = \frac{Cov(X,Y)}{\sqrt{Var(x) \cdot Var(Y)}}
\end{displaymath}

\paragraph{Proposici\'on.} Sea $X$ una variable aleatoria continua con
funci\'on de densidad $f(x)$ y $H(x)$ una funci\'on diferenciable. Entonces:
\begin{displaymath}
E(H(X)) = \int_{-\infty}^{\infty} H(x) \cdot f(x) dx
\end{displaymath}


%---------------------------------------------------------------------------

\section{La variable aleatoria de Bernoulli}

\paragraph{Definici\'on.} La variable aleatoria discreta Bernouilli, $X$, se 
utiliza para modelar fen\'omenos que solamente pueden tomar dos estados, 
$0$ y $1$, con probabilidades $p$ y $(1-p)$ respectivamente. La notaremos 
como $X \sim Ber(p)$:
\begin{displaymath}
P(X=0) = (1 - p) \qquad   P(X=1) = p \qquad p \in [0,1]
\end{displaymath}
 
\paragraph{Esperanza.} La esperanza de una variable aleatoria Bernouilli $X \sim Ber(p)$ 
es $p$. Este resultado es la aplicaci\'on directa de la definici\'on de esperanza 
para una variable aleatoria discreta:
\begin{displaymath}
E(X) = \sum_{i} i \cdot P(X=i) = 1 \cdot p + 0 \cdot (1-p) = p
\end{displaymath}

\paragraph{Varianza.} La varianza de una variable aleatoria Bernouilli $X \sim Ber(p)$ 
es $p \cdot (1-p)$. Este resultado es la aplicaci\'on directa de la definici\'on 
de varianza para una variable aleatoria discreta:
\begin{displaymath}
Var(X)= \sum_{i} (i-E(X))^2 \cdot P(X=i) = (1-p)^2 \cdot p + (-p)^2 \cdot (1-p) = p \cdot (1-p)
\end{displaymath}
 
\paragraph{Simulaci\'on.} La simulaci\'on de una variable Bernouilli 
$X \sim Ber(p)$ la realizamos de la siguiente forma:
\begin{displaymath}
x= \left\{
\begin{array}{cc}
0 & u \in [0,1-p) \cr
1 & u \in [1-p,1]
\end{array}
\right.
\qquad u \sim U[0,1]
\end{displaymath}

%---------------------------------------------------------------------------

\section{La variable aleatoria Binomial}

\paragraph{Definici\'on.} La suma de $n$ variables aleatorias Bernoulli, $Ber(p)$,  
independientes e id\'enticamente distribuidas es una variable aleatoria discreta, 
$X$ que llamamos Binomial, $X \sim B(n,p)$.
\begin{displaymath}
P(X=k) = {n \choose k} \cdot p^k \cdot (1-p)^{n-k} \qquad {n \choose k} = \frac{n!}{k! \cdot (n-k)!}
\qquad k \in \{0, \cdots, n\}
\end{displaymath}

\paragraph{Esperanza.} La esperanza de una variable aleatoria Binomial 
$X \sim B(n,p)$ es:
\begin{displaymath}
E(X) = n \cdot p
\end{displaymath}

\paragraph{Varianza.} La varianza de una variable aleatoria Binomial 
$X \sim B(n,p)$ es:
\begin{displaymath}
Var(X)= n \cdot p \cdot (1-p)
\end{displaymath}

\paragraph{Proposici\'on.} El Teorema Central del L\'imite nos permite, en el
caso de $n$ grandes, aproximar la distribuci\'on discreta Binomial por una 
distribuci\'on continua Normal:
\begin{displaymath}
B(n,p) \approx N\left(n p, n p (1-p)\right)
\end{displaymath}


%---------------------------------------------------------------------------

\section{La variable aleatoria Normal}

\paragraph{Definici\'on.} Decimos que una variable aleatoria continua $Z$ es 
una Normal con media $\mu$ y desviaci\'on est\'andar $\sigma$, 
$Z \sim N(\mu, \sigma^2)$ si tiene la siguiente funci\'on de densidad:
\begin{displaymath}
f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}
\end{displaymath}

\paragraph{Esperanza.} La esperanza de una variable aleatoria Normal 
$X \sim N(\mu,\sigma^2)$ es:
\begin{displaymath}
E(X) = \mu
\end{displaymath}

\paragraph{Varianza.} La varianza de una variable aleatoria Normal 
$X \sim N(\mu,\sigma^2)$ es:
\begin{displaymath}
Var(X)= \sigma^2
\end{displaymath}

\paragraph{Simulaci\'on.} Para la generaci\'on de una realizaci\'on, $z$, de 
una variable aleatoria normal $Z \sim N(\mu, \sigma^2)$ utilizamos el siguiente 
algoritmo:
\begin{displaymath}
z = \mu + \sigma\cdot \sqrt{-2 ln(u_1)} \cdot cos(2 \pi \cdot u_2)
\qquad u_1, u_2 \sim U[0,1]
\end{displaymath}

\paragraph{Definici\'on.} Decimos que una variable aleatoria continua 
$n$-dimensional, $Z$, es una Normal con media $\vec{\mu}$ y matriz de 
covarianza $\Sigma$, $Z \sim N(\vec{\mu}, \Sigma)$, si tiene la 
siguiente funci\'on de densidad:
\begin{displaymath}
f(\vec{x}) = \frac{1}{\sqrt{(2 \pi)^n \left| \Sigma \right|}} 
exp\left\{-\frac{1}{2}\left(\vec{x}-\vec{\mu})^{\top} \Sigma^{-1} (\vec{x}-\vec{\mu}) \right)\right\}
\end{displaymath}
donde $\left|\Sigma\right|$ es el determinante de la matriz de covarianzas 
$\Sigma$, y $\Sigma^{-1}$ es la inversa de la matriz $\Sigma$.

\paragraph{Simulaci\'on.} Para la generaci\'on de una realizaci\'on, $\vec{z}$, 
de una variable aleatoria normal $Z \sim N(\vec{\mu}, \Sigma)$ utilizamos el 
siguiente algoritmo:
\begin{displaymath}
\vec{z} = \vec{\mu} + \Sigma^{1/2} \vec{x}
\qquad x_i \sim N[0,1]
\end{displaymath}
La matriz $\Sigma^{1/2}$ la calculamos usando el algoritmo de Cholesky. Sabemos 
que existe soluci\'on debido a que $\Sigma$ es definida positiva por tratarse
de una matriz de covarianzas.

%---------------------------------------------------------------------------

\section{C\'alculo de la raiz de una matriz}
\label{apendix:sqrtmat}

\paragraph{Definici\'on.}
Diremos que 2 matrices $A$ y $B$ de orden $n$ son semejantes si existe una 
matriz, $P$, de orden $n$ con $det(P) \neq 0$ tal que 
$B = P^{-1} \cdot A \cdot P$.


\paragraph{Proposici\'on.} Si dos matrices $A$ y $B$ son semejantes 
($B = P^{-1} \cdot A \cdot P$) entonces:
\begin{displaymath}
det(A) = det(B)
\end{displaymath}
\begin{displaymath}
B^n = P^{-1} \cdot A^{n} \cdot P
\end{displaymath}

\paragraph{Definici\'on.} 
Diremos que una matriz $A$ de orden $n$ es diagonalizable si es semejante a una 
matriz diagonal $D$, o sea, $A = P^{-1} \cdot D \cdot P$ siendo $det(D) \neq 0$.

\paragraph{Proposici\'on.} 
Para que una matriz $A$ sea diagonalizable es necesario y suficiente que:
\begin{itemize}
\item Los valores propios de $A$ sean todos reales
\item Los $n$ vectores propios de $A$ sean independientes
\end{itemize}

\paragraph{Proposici\'on.}
Si una matriz $A$ es diagonalizable ($A = P^{-1} \cdot D \cdot P$) entonces: 
\begin{itemize}
\item $D$ es una matriz diagonal compuesta por los valores propios de la matriz $A$
\item $P$ es la matriz formada por los vectores propios de la matriz $A$
\end{itemize}

\paragraph{Resultado.}
Sea $A$ la ra\'iz $n$-esima de una matriz diagonalizable $B$. Entonces:
\begin{displaymath}
A^n = B = P^{-1} \cdot D \cdot P 
\Longrightarrow  
A = \sqrt[n]{B} = P^{-1} \cdot \sqrt[n]{D} \cdot P
\end{displaymath} 

%---------------------------------------------------------------------------

\section{Algoritmo de la c\'opula normal}

Sea $\Sigma_1$ la matriz de correlaci\'on a cumplir por la c\'opula. Observemos 
que se trata tambi\'en de la matriz de covarianzas al valer $1$ los elementos
de la diagonal.
\begin{displaymath}
\Sigma_1 = \left( 
\begin{array}{cccc}
1          & \rho_{12} & \ldots & \rho_{1n} \cr
\rho_{21} & 1          & \ldots & \rho_{2n} \cr
\vdots    & \vdots    & \ddots & \vdots   \cr
\rho_{n1} & \rho_{n2} & \ldots & 1
\end{array}
\right)
\end{displaymath}

\paragraph{Paso 1.} Creamos la matrix de covarianzas $\Sigma_2$ transformando 
la matriz $\Sigma_1$ componente a componente:
\begin{displaymath}
\Sigma_2 = \left( 
\begin{array}{cccc}
2 sin(\frac{\pi}{6})           & 2 sin(\rho_{12} \frac{\pi}{6}) & \ldots & 2 sin(\rho_{1n} \frac{\pi}{6})\cr
2 sin(\rho_{21} \frac{\pi}{6}) & 2 sin(\frac{\pi}{6})           & \ldots & 2 sin(\rho_{2n} \frac{\pi}{6})\cr
\vdots                          & \vdots                          & \ddots  & \vdots   \cr
2 sin(\rho_{n1} \frac{\pi}{6}) & 2 sin(\rho_{n2} \frac{\pi}{6}) & \ldots & 2 sin(\frac{\pi}{6})
\end{array}
\right)
\end{displaymath}

\paragraph{Paso 2.} A la matriz $\Sigma_2$ le aplicamos el algoritmo de 
Cholesky para obtener la matrix triangular inferior $B$ cumpliendo 
$\Sigma_2 = B \cdot B^{\top}$:
\begin{displaymath}
B = 
\left(
\begin{array}{cccc}
b_{11}   & 0        & \ldots & 0       \cr
b_{21}   & b_{22}   & \ldots & 0       \cr
\vdots  & \vdots  & \ddots & \vdots \cr
b_{n1}   & b_{n2}   & \ldots & b_{nn}
\end{array}
\right)
\end{displaymath}

\paragraph{Paso 3.} Simulamos $n$ variables aleatorias $N(0,1)$ independientes:
\begin{displaymath}
\vec{Y}^{\top}=(Y_1, \cdots, Y_n)^{\top} \qquad Y_k \sim N(0,1) \textrm{ independientes}
\end{displaymath}

\paragraph{Paso 4.} Simulamos una variable n-dimensional $Z \sim N(\vec{0}, \Sigma_2)$
haciendo:
\begin{displaymath}
\vec{Z}^{\top} = 
\left(
\begin{array}{c}
Z_1 \cr
\vdots \cr
Z_n
\end{array}
\right) 
=
\left(
\begin{array}{cccc}
b_{11}   & 0        & \ldots & 0       \cr
b_{21}   & b_{22}   & \ldots & 0       \cr
\vdots  & \vdots  & \ddots & \vdots \cr
b_{n1}   & b_{n2}   & \ldots & b_{nn}
\end{array}
\right)
\left(
\begin{array}{c}
Y_1 \cr
\vdots \cr
Y_n
\end{array}
\right) 
 = B \cdot \vec{Y}^{\top}
\end{displaymath}

\paragraph{Paso 5.} Finalmente obtenemos la simulaci\'on de la c\'opula, $\vec{X}$.
\begin{displaymath}
\vec{X}^{\top} = (X_1, \cdots, X_n)^{\top} = (\Phi(Z_1), \cdots, \Phi(Z_n))^{\top}
\end{displaymath}
donde $\Phi(x)$ es la funci\'on de distribuci\'on de la Normal est\'andar
\begin{displaymath}
\Phi(x) = \int_{-\infty}^{x} \frac{1}{\sqrt{2 \pi}} e^{-\frac{t^2}{2}} dt
\end{displaymath}

